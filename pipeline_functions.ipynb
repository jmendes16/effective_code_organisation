{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a580c40-93b1-4201-828b-f18d6f191939",
   "metadata": {},
   "source": [
    "# Modularising Python Code for Data Pipelines\n",
    "\n",
    "The purpose of this notebook is to guide you through some basic data pipeline python code and how we might break it down into functions.  \n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure you have this notebook stored in the same folder as customer data.  \n",
    "When creating a data pipeline we are normally taking data from one place to another, source to destination.  Our source, initially, will be our customer data csv.  Our destination will be a postgres database instance hosted on supabase.com.  If you have not used supabase before please create an account and a project called 'my_business_insights', NB: when creating your project you will be asked for a password, it is very important that you remember this password as you will need it to interact with your database.  All other connection parameters can be found from the menu pane on the left 'project settings'>'Database'>'connection parameters'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e183d3e-f90f-4bd1-85a7-7f4109010e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your supabase parameters\n",
    "POSTGRES_HOST = \"your_host\"\n",
    "POSTGRES_USER = \"your_user\"\n",
    "POSTGRES_PASSWORD = \"your_password\"\n",
    "POSTGRES_DATABASE = \"postgres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac221763-ce96-4e81-9ccd-b810dcd71da1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "###FOR INSTRUCTOR USE IN LIVE DELIVERY###\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# read the configuration file\n",
    "config.read('my_config.ini')\n",
    "\n",
    "# your supabase parameters\n",
    "POSTGRES_HOST = config.get('DB','host')\n",
    "POSTGRES_USER = config.get('DB','username')\n",
    "POSTGRES_PASSWORD = config.get('DB','password')\n",
    "POSTGRES_DATABASE = config.get('DB','db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e5d4b5-a3c0-4f17-b68c-74e5daa0282d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\joel.mendes\\anaconda3\\envs\\mv\\lib\\site-packages (2.0.30)\n",
      "Requirement already satisfied: pandas in c:\\users\\joel.mendes\\anaconda3\\envs\\mv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\joel.mendes\\anaconda3\\envs\\mv\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\joel.mendes\\anaconda3\\envs\\mv\\lib\\site-packages (from sqlalchemy) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\joel.mendes\\anaconda3\\envs\\mv\\lib\\site-packages (from sqlalchemy) (3.0.3)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\joel.mendes\\anaconda3\\envs\\mv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joel.mendes\\anaconda3\\envs\\mv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joel.mendes\\anaconda3\\envs\\mv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joel.mendes\\anaconda3\\envs\\mv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joel.mendes\\anaconda3\\envs\\mv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# install necessary packages in your environment\n",
    "!pip install sqlalchemy pandas psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24737a-e0fb-4cb0-ac76-e7f4c4ba0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from sqlalchemy import create_engine, text, Table, MetaData, insert, select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653bdece-a581-4fa3-a3c6-7a4fad49a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv into python\n",
    "data = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# establish connection to destination database\n",
    "conn_string = f'postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:5432/{POSTGRES_DATABASE}'\n",
    "engine = create_engine(conn_string)\n",
    "\n",
    "# create table sql query\n",
    "create_table_sql = text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS customer_data (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        name VARCHAR(255),\n",
    "        postcode VARCHAR(9)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "# context manager manage connection to database\n",
    "# this block ensures the table exists in the database before we try and write to it\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        conn.execute(create_table_sql)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        conn.rollback()\n",
    "\n",
    "# get the table from the database\n",
    "metadata = MetaData()\n",
    "customer_table = Table('customer_data', metadata, autoload_with=engine)\n",
    "# format data for writing to database\n",
    "data_dict = data.to_dict('records')\n",
    "\n",
    "\n",
    "# context manager manages connection to database\n",
    "# this block writes the data to the database\n",
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        conn.execute(insert(customer_table),\n",
    "                    data_dict)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6cd90-a86e-4c2f-bab4-ced12f7728d3",
   "metadata": {},
   "source": [
    "Now try breaking down this process into separate functions.  The answers are in the cell block below, click on the cell to reveal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ee9441-37fc-4e33-a640-18225d888a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code in here.  \n",
    "\n",
    "def get_data(filename_csv):\n",
    "    ...\n",
    "\n",
    "def get_sql_engine(user, passwd, host):\n",
    "    ...\n",
    "\n",
    "def connection_manager(sql_engine, query, data=[]):\n",
    "    ...\n",
    "\n",
    "def insert_data(table_name, engine):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae5563-9d30-4b2d-ada3-2fd08526d942",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "###ANSWERS###\n",
    "def get_data(filename_csv):\n",
    "    data = pd.read_csv(filename_csv)\n",
    "    return data.to_dict('records')\n",
    "\n",
    "def get_sql_engine(user, passwd, host, database):\n",
    "    conn_string = f'postgresql+psycopg2://{user}:{passwd}@{host}:5432/{database}'\n",
    "    return create_engine(conn_string)\n",
    "\n",
    "def connection_manager(sql_engine, query, data =[]):\n",
    "    with sql_engine.connect() as conn:\n",
    "        try:\n",
    "            result = conn.execute(query, data)\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            conn.rollback()\n",
    "    return result\n",
    "\n",
    "def insert_data(table_name, engine):\n",
    "    metadata = MetaData()\n",
    "    table = Table(table_name, metadata, autoload_with=engine)\n",
    "    return insert(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7e818c-5d3c-4bcc-a570-9aea52494014",
   "metadata": {},
   "source": [
    "Now that we've broken up our code into easy manageable chunks we can run our pipeline using the following neater script.  \n",
    "NB: The last two functions could be rewritten to make use of df.to_sql() this is potentially more efficient.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b58b5-39cc-4e5d-bd4d-0affaa3cdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this ensures the table exists in the database before we try and write to it\n",
    "engine = get_sql_engine(POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_DATABASE)\n",
    "\n",
    "# create table sql query\n",
    "create_table_sql = text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS customer_data (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        name VARCHAR(255),\n",
    "        postcode VARCHAR(9)\n",
    "    );\n",
    "    \"\"\")\n",
    "connection_manager(engine, create_table_sql)\n",
    "\n",
    "# insert the data into the table\n",
    "connection_manager(engine,\n",
    "                   insert_data('customer_data', engine),\n",
    "                   get_data('customer_data.csv')\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da27324-8a28-4441-8f93-61dbe36edf62",
   "metadata": {},
   "source": [
    "This code is reuseable, makes improvements and makes troubleshooting easier to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881432ea-8f23-4b1d-8271-f7a7f0eea657",
   "metadata": {},
   "source": [
    "## Adding in Geolocation\n",
    "\n",
    "Next we will look at adding in geolocation data by making use of the postcodes.io API.  Lets have a look at how it works.\n",
    "https://postcodes.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b2441-55ac-46bc-8173-f3c093e428a7",
   "metadata": {},
   "source": [
    "Here we'll make a call to the postcodes API and have a look at the response.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618caa82-cbb8-40d0-becf-39c87f39ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.postcodes.io/postcodes\"\n",
    "params = {\n",
    "\t\"postcodes\": [\"S1 2BP\",\"W2 6LG\"]\n",
    "}\n",
    "response = requests.post(url, json = params)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff49c50-7e83-40c2-82ca-bab305ee4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(response.json(),indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af1c6a-1a6a-4455-aa25-295aa16713f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json()['result'][0]['result']['longitude'], response.json()['result'][0]['result']['latitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11930acf-7779-40e8-8b65-f137a479173d",
   "metadata": {},
   "source": [
    "Now that we understand what the response looks like from the API we can write some functions to add this information to our database.  \n",
    "Try breaking down this process into separate functions. The answers are in the cell block below, click on the cell to reveal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b9028-2a0e-4639-b549-d78c01ca9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop us overloading the API we're going to limit the calls to just 10 postcodes at a time.  \n",
    "def get_10_postcodes(engine):\n",
    "    metadata = MetaData()\n",
    "    customers = Table('customer_data', metadata, autoload_with=engine)\n",
    "    location = Table('location_data', metadata, autoload_with=engine)\n",
    "    result = ...\n",
    "    return result\n",
    "\n",
    "# Next we'll get our data from the API.  \n",
    "def get_long_lat(postcode_list):\n",
    "    ...\n",
    "    \n",
    "# Then we need to format it so we only get the bits we're interested in.  \n",
    "def extract_long_lat(data):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a13972-fde1-4384-ae77-10816a9f7e7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "###ANSWERS###\n",
    "# To stop us overloading the API we're going to limit the calls to just 10 postcodes at a time.  \n",
    "def get_10_postcodes(engine):\n",
    "    metadata = MetaData()\n",
    "    customers = Table('customer_data', metadata, autoload_with=engine)\n",
    "    location = Table('location_data', metadata, autoload_with=engine)\n",
    "    result = select(customers.c.postcode).where(\n",
    "        customers.c.postcode.notin_( # excluding ones we've already got geolocation for\n",
    "            select(location.c.postcode)\n",
    "        )).limit(10) # only 10 postcodes at a time\n",
    "    return result\n",
    "\n",
    "# Next we'll get our data from the API.  \n",
    "def get_long_lat(postcode_list):\n",
    "    url = \"https://api.postcodes.io/postcodes\"\n",
    "    params = {\n",
    "    \t\"postcodes\": postcode_list\n",
    "    }\n",
    "    response = requests.post(url, json = params)\n",
    "    return response.json()\n",
    "\n",
    "# Then we need to format it so we only get the bits we're interested in.  \n",
    "def extract_long_lat(data):\n",
    "    extracted_data = []\n",
    "    for item in data['result']:\n",
    "        if 'result' in item and item['result']:\n",
    "            extracted_data.append({\n",
    "                'postcode': item['result'].get('postcode'),\n",
    "                'longitude': item['result'].get('longitude'),\n",
    "                'latitude': item['result'].get('latitude')\n",
    "            })\n",
    "    return pd.DataFrame(extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1323c538-0241-4a4c-9109-86b90b961e5a",
   "metadata": {},
   "source": [
    "Now we have our functions we can put them together into a script and update our database.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635e126-acf1-4102-b03d-e6767a761c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this ensures the table exists in the database before we try and write to it\n",
    "engine = get_sql_engine(POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_HOST, POSTGRES_DATABASE)\n",
    "\n",
    "# making sure our target table is ready\n",
    "create_table_sql = text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS location_data (\n",
    "        postcode VARCHAR(9) PRIMARY KEY,\n",
    "        longitude FLOAT,\n",
    "        latitude FLOAT\n",
    "    );\n",
    "    \"\"\")\n",
    "connection_manager(engine, create_table_sql)\n",
    "\n",
    "# We grab our postcodes\n",
    "result = connection_manager(engine,\n",
    "                   get_10_postcodes(engine)\n",
    "                  ).fetchall()\n",
    "postcodes = [i[0] for i in result]\n",
    "\n",
    "# We extract the data from the API\n",
    "data = get_long_lat(postcodes)\n",
    "\n",
    "# We filter out all the unnecessary additional information\n",
    "df = extract_long_lat(data)\n",
    "\n",
    "# We load the data into our database\n",
    "connection_manager(engine,\n",
    "                   insert_data('location_data',engine),\n",
    "                   df.to_dict('records')\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41638aaf-0600-4317-888c-059ae7577d58",
   "metadata": {},
   "source": [
    "This has now added data to our second table in our database.  Here is a link to the schema we're trying to produce https://lucid.app/lucidchart/8f0f8fd2-e3ae-4d68-891a-dca08ea9da38/edit?viewport_loc=1258%2C-150%2C1744%2C781%2C0_0&invitationId=inv_81a74f07-50c9-4088-9478-179449341de7.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b1117",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
